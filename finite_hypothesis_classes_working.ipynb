{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70352e3c-2713-4f7f-889e-a18c1bf280e6",
   "metadata": {},
   "source": [
    "# Finite Hypothesis Classes\n",
    "\n",
    "We are going to go implement some bare-bones supervised machine learning models with finite hypotheses classes on the famous MNIST dataset for handwritten digit recognition, and be able to classify with 90% accuracy between the numbers '1' and '2'. This is a coding complement for the concepts in the first chapter of the 'Learning From Data' textbook (Abu-Mostafa, Magdon-Ismail, Lin), and so we are choosing between '1' and '2' to match the first chapters's assumption of a binary output. Our model can't say anything about the other numbers. \n",
    "\n",
    "The MNIST contains input data entries that look like this - user-handwritten numbers, in 28x28 matrices where one cell in the matrix is the pixel of the image, and the value of the cell is the pixel intensity in greyscale, where 0 = black and 255 = white. \n",
    "\n",
    "<img title=\"a title\" alt=\"Alt text\" src=\"/MnistExamplesModified.png\">\n",
    "\n",
    "\n",
    "## Basic setup of the learning model + math \n",
    "\n",
    " Let's see Figure 1.2 in the textbook: \n",
    "\n",
    "<img title=\"a title\" alt=\"Alt text\" src=\"/figure_1.2.png\">\n",
    "\n",
    "In this case, our $X$ will be all possible variations that users can fill out the 28x28 matrix of pixels, and $Y$ is the set of numbers from 0-9. Our training examples of $\\{(\\vec{x_1}, y_ ), ..., (\\vec{x_n}, y_n)\\}$ will have a $\\vec{x_i}$ be the user input of their handwritten number, and the $y_i$ is the number they are writing. \n",
    "\n",
    "We assume there is some ground truth function $f$ that always correctly classifies the handwritten number ($y \\in Y$), and we try to choose a $g$ that is classifying the handwriting well enough that we say it approximates $f$ - and what are choosing $g$ out of? The hypothesis class! The hypothesis class is a set of functions, and we go about choosing the final hypothesis with an algorithm that systematically goes through the different hypotheses and chooses the one with the lowest in-sample error. \n",
    "\n",
    "Recall that we are using finite hypothesis classes. Specifically, these five: \n",
    "\n",
    "1) $H_\\text{binary} = \\{ \\vec{x} \\mapsto +1, \\vec{x} \\mapsto -1 \\}$ : this hypothesis class has two hypotheses. The first hypothesis sends all of the inputs to the output -1, and the second hypothesis sends all of the inputs to the output of 1. \n",
    "As code:\n",
    "``` \n",
    "H_binary = [lambda x: 1, lambda x: -1]\n",
    "```\n",
    "\n",
    "2) $H_\\text{axis} = \\{ \\vec{x} \\mapsto \\text{sign}(x_i): i \\in [d] \\}$ : this hypothesis classes has $d$ hypotheses where $d$ is the dimension-of/amount-of-entries-in the inputs $\\vec{x} \\in X$. If $d$ = 2, then our points will be on the $x_1,x_2$ graph (commonly referred to as the $x, y$ graph) and there will be two hypotheses. The first hypothesis will 'select' the $x_1$-axis ($x$-axis) such that all points above the $x$-axis are 1 and below the $x$-axis are -1. The second hypothesis will select the $x_2$-axis ($y$-axis) such that points to the right are 1, to the left are -1. The concept extends as $d>2$. \n",
    "As code: \n",
    "```\n",
    "H_axis = lambda d: [lambda x, i=i: sign(x[i]) for i in range(d)]\n",
    "```\n",
    "\n",
    "\n",
    "3) $H_\\text{axis2} = \\{ \\vec{x} \\mapsto \\sigma \\text{sign}(x_i) : \\sigma \\in \\{ 1, -1 \\}, i \\in [d] \\}$ : \n",
    "has $2d$ hypotheses. Extending on the example given above, this hypothesis class enables us to choose one axis, but, say for the $x_1$-axis, we can choose points below the $x_1$-axis to be 1 and above to be -1. Thus, we have twice as many hypotheses than in $H_\\text{axis}$. \n",
    "As code: \n",
    "```\n",
    "H_axis2 = lambda d: [lambda x, i=i, sigma=sigma: sigma * sign(x[i]) for i in range(d) for sigma in [+1, -1]]\n",
    "```\n",
    "\n",
    "\n",
    "4) $H_\\text{multiaxis2} = \\{ \\vec{x} \\mapsto \\text{sign}(\\sum_{j=1}^d \\sigma_j \\text{sign}(x_j)) : \\sigma_i \\in \\{1, -1\\}, i \\in [d] \\}$ : has $2^d$ hypotheses - for each element $x_i$ of the input vector $\\vec{x}$, there are two possible choices of weight, either  +1 or  -1 . Since there are $d$ elements, and each can independently take one of two values, the total number of possible combinations, and therefore unique hypotheses, is  $2^d$. \n",
    "As code: \n",
    "```\n",
    "H_multiaxis2 = lambda d: [lambda x, sigma=sigma: np.sign(np.sum(sigma * np.sign(x)))\n",
    "                          for sigma in np.array(np.meshgrid(*[[-1, 1]] * d)).T.reshape(-1, d)]\n",
    "```\n",
    "\n",
    "\n",
    "5) $H_\\text{multiaxis3} = \\{ \\vec{x} \\mapsto \\text{sign}(\\sum_{j=1}^d \\sigma_j \\text{sign}(x_j) ): \\sigma_i \\in \\{1, 0, -1 \\}, i \\in [d] \\} $ : \n",
    "has $3^d$ hypotheses - it is similar to the hypothesis above with the only change being that there is an additional weight of '0'. \n",
    "As code:\n",
    "```\n",
    "H_multiaxis3 = lambda d: [lambda x, sigma=sigma: np.sign(np.sum(sigma * np.sign(x)))\n",
    "                          for sigma in np.array(np.meshgrid(*[[-1, 0, 1]] * d)).T.reshape(-1, d)]\n",
    "```\n",
    "\n",
    "\n",
    "Because our hypothesis classes are finite, we can simply try every hypothesis on our data set and see which hypothesis has the lowest in-sample error - the 'try-everything-algorithm' TEA for $A$ - implementation upcoming below. \n",
    "\n",
    "With that, we have finished mapping the different components of the MNIST to this basic learning set-up, and are ready to start coding it up. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229b33e5-37f7-4023-872b-b06e97dfbf23",
   "metadata": {},
   "source": [
    "## Part 0: Coding imports + helper functions\n",
    "\n",
    "This section contains imports and helper functions used by the code in parts 1+.\n",
    "It is safe to skip reading the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75b19d3c-f3d6-445a-b7c7-00febf80477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "import time\n",
    "\n",
    "# make numpy random numbers deterministic\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# enable plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edf84e74-8bbc-432f-8b3a-282623776629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign(a):\n",
    "    '''\n",
    "    Convert a boolean value into +/- 1.\n",
    "\n",
    "    >>> sign(12.5)\n",
    "    1\n",
    "    >>> sign(-12.5)\n",
    "    -1\n",
    "    '''\n",
    "    if a > 0:\n",
    "        return 1\n",
    "    if a <= 0:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def set_exp(xs, p):\n",
    "    '''\n",
    "    Compute the \"set exponential\" function.\n",
    "\n",
    "    For efficiency, this function is a generator.\n",
    "    This means that large sets will never be explicitly stored,\n",
    "    and this function will always use O(1) memory.\n",
    "\n",
    "    The doctests below first convert the generator into a list for visualization.\n",
    "\n",
    "    >>> list(set_exp([-1, +1], 0))\n",
    "    []\n",
    "    >>> list(set_exp([-1, +1], 1))\n",
    "    [[-1], [1]]\n",
    "    >>> list(set_exp([-1, +1], 2))\n",
    "    [[-1, -1], [1, -1], [-1, 1], [1, 1]]\n",
    "    >>> list(set_exp([-1, +1], 3))\n",
    "    [[-1, -1, -1], [1, -1, -1], [-1, 1, -1], [1, 1, -1], [-1, -1, 1], [1, -1, 1], [-1, 1, 1], [1, 1, 1]]\n",
    "\n",
    "    Observe that the length grows exponentially with the power.\n",
    "\n",
    "    >>> len(list(set_exp([-1, +1], 4)))\n",
    "    16\n",
    "    >>> len(list(set_exp([-1, +1], 5)))\n",
    "    32\n",
    "    >>> len(list(set_exp([-1, +1], 6)))\n",
    "    64\n",
    "    >>> len(list(set_exp([-1, +1], 7)))\n",
    "    128\n",
    "    >>> len(list(set_exp([-1, +1], 8)))\n",
    "    256\n",
    "    '''\n",
    "    assert(len(xs) > 0)\n",
    "    assert(p >= 0)\n",
    "    assert(type(p) == int)\n",
    "    if p == 1:\n",
    "        for x in xs:\n",
    "            yield [x]\n",
    "    elif p > 1:\n",
    "        for x in xs:\n",
    "            for ys in set_exp(xs, p - 1):\n",
    "                yield ys + [x]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659e448f-31d7-4870-8904-5268472c6739",
   "metadata": {},
   "source": [
    "## Part 1: Hypothesis Classes\n",
    "\n",
    "We've already translated the mathematical definitions of the finite hypothesis classes into python code above - let us warm up and see how $H_\\text{binary}$ and $H_\\text{axis}$ act on a simple example vector `x_ex`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8de7c07-8eb3-44b4-9316-7566fed7ba26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H_binary\n",
      "x =  [ 12.5 -12.5   1.2  -1.2]\n",
      "ypred=1\n",
      "ypred=-1\n"
     ]
    }
   ],
   "source": [
    "# The H_binary hypothesis class is easy to represent in code as a list of anonymous functions since there are only 2 functions\n",
    "H_binary = lambda d: [lambda x: +1, lambda x: -1]\n",
    "# The code below shows how to use one of these functions on example datapoint x\n",
    "# Then we apply every hypothesis h in the hypothesis class to the sample.\n",
    "\n",
    "x_ex = np.array([12.5, -12.5, 1.2, -1.2])\n",
    "\n",
    "print('H_binary')\n",
    "print('x_ex = ', x_ex)\n",
    "# passing in d = 4 since vector x has 4 elements\n",
    "for h in H_binary(4):\n",
    "    ypred = h(x_ex)\n",
    "    print(f'ypred={ypred}')\n",
    "    \n",
    "# we're expecting 2 outputs for the 2 functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7a19dae-4953-4b87-9ef1-bc787d288b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H_axis\n",
      "x =  [ 12.5 -12.5   1.2  -1.2]\n",
      "ypred=1\n",
      "ypred=-1\n",
      "ypred=1\n",
      "ypred=-1\n"
     ]
    }
   ],
   "source": [
    "H_axis = lambda d: [lambda x, i=i: sign(x[i]) for i in range(d)]\n",
    "\n",
    "x = np.array([12.5, -12.5, 1.2, -1.2])\n",
    "\n",
    "print('H_axis')\n",
    "print('x = ', x)\n",
    "for h in H_axis(4):\n",
    "    ypred = h(x)\n",
    "    print(f'ypred={ypred}')\n",
    "\n",
    "# expecting d outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b44d063c-4ac7-4b1e-9900-fc25f11f6407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# notice that the size of H_binary doesn't change based on the number of dimensions,\n",
    "# but that the size of H_axis does\n",
    "print(len(H_binary(5)))\n",
    "print(len(H_axis(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a748674",
   "metadata": {},
   "source": [
    "We can make a table to see how these different hypothesis classes grow in size as $d$ increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60e44e40-0665-4b97-b150-7eab0be5e993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  d     len(H_binary(d))      len(H_axis(d))     len(H_axis2(d))     len(H_multiaxis2(d))     len(H_multiaxis3(d))\n",
      "  1                    2                   1                   2                        2                        3\n",
      "  2                    2                   2                   4                        4                        9\n",
      "  3                    2                   3                   6                        8                       27\n",
      "  4                    2                   4                   8                       16                       81\n",
      "  5                    2                   5                  10                       32                      243\n",
      "  6                    2                   6                  12                       64                      729\n",
      "  7                    2                   7                  14                      128                     2187\n",
      "  8                    2                   8                  16                      256                     6561\n",
      "  9                    2                   9                  18                      512                    19683\n",
      " 10                    2                  10                  20                     1024                    59049\n"
     ]
    }
   ],
   "source": [
    "H_binary = lambda d: [lambda x: +1, lambda x: -1]\n",
    "H_axis = lambda d: [lambda x, i=i: sign(x[i]) for i in range(d)]\n",
    "\n",
    "H_axis2 = lambda d: [lambda x, i=i, sigma=sigma: sigma * sign(x[i]) for i in range(d) for sigma in [+1, -1]]\n",
    "\n",
    "H_multiaxis2 = lambda d: [lambda x, sigma=sigma: np.sign(np.sum(sigma * np.sign(x)))\n",
    "                          for sigma in np.array(np.meshgrid(*[[-1, 1]] * d)).T.reshape(-1, d)]\n",
    "\n",
    "\n",
    "H_multiaxis3 = lambda d: [lambda x, sigma=sigma: np.sign(np.sum(sigma * np.sign(x)))\n",
    "                          for sigma in np.array(np.meshgrid(*[[-1, 0, 1]] * d)).T.reshape(-1, d)]\n",
    "\n",
    "\n",
    "# the following code prints a nice table showing the size of the finite hypothesis classes in different dimensions\n",
    "# you will know your implementations above are correct if the sizes match the formulas we derived in the notes \n",
    "print(f'  d {\"len(H_binary(d))\":>20}{\"len(H_axis(d))\":>20}{\"len(H_axis2(d))\":>20}{\"len(H_multiaxis2(d))\":>25}{\"len(H_multiaxis3(d))\":>25}')\n",
    "for d in range(1, 11):\n",
    "    print(f' {d:2} {len(H_binary(d)):20d}{len(H_axis(d)):20d}{len(H_axis2(d)):20d}{len(H_multiaxis2(d)):25d}{len(H_multiaxis3(d)):25d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d195ae-06fb-4703-adaf-dbf27e019c19",
   "metadata": {},
   "source": [
    "## Part 2: Loading the MNIST Data\n",
    "\n",
    "This section loads the dataset and starts using Scikit-learn and NumPy! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63ff2db",
   "metadata": {},
   "source": [
    "We're loading the MNIST data such that we directly map images to be vectors in $X$, and outputs to $y$. Notice that we're using capital $X$ as the name of our variable to show that it contains a matrix, and $y$ is a vector. We have a matrix for $X$ because there will be $n$ vectors/inputs with dimension/features-of-inputs $d$, so $X$ is a $n$x$d$ matrix. Meanwhile, $y$ only outputs 1 number, either '1' or '-1', so it has $n$x1 shape. \n",
    "\n",
    "The features of $X$ is just each pixel in the image. Our images are 28x28 pixel matrices, so as vectors, the dimension will be 28 $\\cdot$ 28 = 784 features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "982aca51-8b83-4a41-bcbe-02fe09d9fca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape=(70000, 784)\n",
      "y.shape=(70000,)\n"
     ]
    }
   ],
   "source": [
    "# scikit learn has built-in functions for loading lots of standard datasets\n",
    "# the MNIST dataset is small by machine learning standards,\n",
    "# but it still takes 10-20 seconds to load on my machine\n",
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "\n",
    "#print the shape of X and y\n",
    "print(f\"X.shape={X.shape}\") # shape = N x d\n",
    "print(f\"y.shape={y.shape}\") # shape = N\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe49e67",
   "metadata": {},
   "source": [
    "We will now adjust the dataset so that we can have a binary output by simply looking at how the input vectors in $X$ are labeled in the corresponding $y$ and remove everything that is not a 1 or 2 - thus creating a 'dataset mask', or filter. \n",
    "\n",
    "Then, we will change $y$ in place to map '1' and '2' in MNIST to be a '1' or '-1' respectively, to match ch.1 of the textbook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "344c227b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape=(14867, 784)\n",
      "y.shape=(14867,)\n"
     ]
    }
   ],
   "source": [
    "#convert MNIST into a binary problem\n",
    "# the idea is that we will label all \"1\"s as +1, all \"2\"s as -1, and delete all the other digits\n",
    "label_positive = '1'\n",
    "label_negative = '2'\n",
    "#logical_or \n",
    "dataset_mask = np.logical_or(y == label_positive, y == label_negative)\n",
    "X = X[dataset_mask]\n",
    "y = y[dataset_mask]\n",
    "y[y == label_positive] = +1\n",
    "y[y == label_negative] = -1\n",
    "y = y.astype(np.int64)\n",
    "\n",
    "#print shape of X and y after removing everything that is not a 1 or 2 \n",
    "print(f\"X.shape={X.shape}\") # shape = N x d\n",
    "print(f\"y.shape={y.shape}\") # shape = N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efbb356",
   "metadata": {},
   "source": [
    "Now that we've set up our data, let's do a quick sanity check and print out the first 5 entries in our new dataset. Remember that these are images stored as vectors, so we have to reshape it into a matrix again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4612a35d-05d4-4f67-9491-e88c3fc21f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0, y[i]=1, x[i]=\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=1, y[i]=-1, x[i]=\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAPeklEQVR4nO2d2W8T1xfHP+Oxx2OP7fGa2FkasrCENSrQ0gekijeqvvQP7XsfUB8qWhAgSIUIpJBFWZ3Eu2fs8Xj7PVT3Nim0v7QlZGjnK0VEYMWe+7333HO+53uCMhwOh/g4VQRO+wP48EnwBHwSPACfBA/AJ8ED8EnwAHwSPACfBA/AJ8EDCB73hYqinOTn+FfiuGKEfxI8AJ8ED8AnwQPwSfAAfBI8AJ8ED8AnwQPwSfAAfBI8AJ8ED8AnwQPwSfAAji3gfQxQVRVFUVAURX4fjUaJx+MEg0H51e122d/fp16vMxwOjy20nRT+NSQoiiIXORAIoGkaqqqSz+eZmZkhEomg6zrRaBTLsnj8+DG2bTMYDOj3+6dKxEdLgpDWVVVFVVUCgQC6rhMKhVBVlXA4TDAYJJlMkslkiEQiRKNRIpEImqYRi8XQdZ1er0en06Hf75/as3yUJGiaJhd8YmKCsbExdF0nnU4Ti8UIh8Mkk0k0TSORSJDJZAiFQvKr0WgQDAbJ5/OUy2WWlpaoVqun9jwfJQmhUIh4PE4kEuHixYtcv36dWCzG+Pg4uVyOaDTK6Ogo0WgU+PW0CCiKcoSE1dVVtra2fBL+DCLUqKqKruuoqoppmmSzWbnYmUwGwzBIJpMkEgl0XUfXdTRNOxLrA4GADFvxeFy+PhwOEwqF5P3woeFpElRVlTs+lUpx/vx5kskk4+PjnD17FsMwyOVy5HI5QqEQhmEQDocBcF1XxvperwdALBYjFosRCoWYnJzEMAyCwSDj4+M0m03a7TbNZvODE+FpEgKBAJFIhEQiwejoKFevXqVQKDA9Pc3CwgLRaJRQKEQw+OtjiMvadV1qtRqdTkdevPBrGIvFYgSDQTKZDLFYjHq9Lk8EgGVZH/w5PUOCoigyXCQSCUzTRNM0JiYmSKfT5HI5JicnyWazJJNJufiHDQjdbpd+v0+9XufNmzfU63V5ElRVZW5ujlQqRSAQIBgMMhgMCIVCaJoms6rTMDR4hgRVVdE0DU3TuHr1KgsLC5imyYULFxgfH0fXdTKZjIz30Wj0yKL1+30ajQbtdpuVlRW+/fZbVldXgd9O1DfffMPMzAzhcFjeA4ZhEI1GMQyDdrv93yZBVLmappHNZpmZmSGZTDI/P88nn3xCMBgkHA4TCLxbaRkOh3Q6HVqtFpVKhTdv3vDy5Uu56w3DoFwuMxgMgN8uaXESgsGgfxIMw2B0dJRYLMb09DRzc3PEYjGSyeSfLpDruriuS7PZZHFxkfX1dTY3N9nb28O2bcLh8JEU1YvwDAmmaXL27FnS6TRXrlzh+vXrsiD7/cUrMBwOabfbNBoN9vb2uHfvHvfv38e2bXZ2drBtm1gshqZpp/FIx4ZnSBCZSzweJ5FIvHPxRM4vRLd+v0+r1aLZbNJoNCiVSuzt7dHpdHAcR2pCgUBACntehGdIME2T2dlZRkZGSKfTb8X+fr9Pu92m1+vRarWo1+u4rsurV69YXl6mWq2yvLxMo9Gg3+/LXF9IGKZpEolETuPR/i88QYKiKCQSCWZmZsjn82Sz2bd2ba/Xw7ZtHMehUqmws7NDo9Hg/v37MgRVKhWazeaRKlnTNEzTlCLeH13spwlPkAC/XrCNRgNd16nX69TrdQKBAN1uVxZc1WpVkrC7u4tlWVQqFSzLwnEcut3uW5K0yICExO1FeIKE4XDIxsYG3333HYlEgosXLzI/P89gMGBvb49arYbjOJTLZRzHwbIsqtUqnU6H/f19yuUy/X4f13Xf+tmqqmIYBoZhoGmaJ+8FT5AAUCqVWFxcRNf1I5rP69evKRaLtNttyuUy7XYbx3GwbftYGo+oL3Rdl1mW1+CZTzUYDOROLpfLbG5u0uv1KJVK8hJ2HAfXden1erLo+n8IBoMy6wqHw/5J+DN0u10sy0JRFJaWllhfX2c4HMpYPxgM5J9/pS8s+gyjo6OYpumT8GcYDAZyd7uu+16aLIqiyFam6Lj9HocJPe7pet/wDAl/BYlEQvYQhAb0R5iZmSGTyWCaJrquoygKg8FAprsHBwccHBxQKpWwLMtv6hwXuVyO69evYxiGlKAFEYqiHAlV09PTTExMkEgkMAwDRVHkXVOpVNjY2GBzc5ONjY0jRd6HhOdJEHKDsLQEAgHi8TjpdFr6iYS88a54n0wmiUQiUoEVcofQnCzLot1u0+l0Ts2D5EkSRIhRVVXG8kQiwdzcHKZpys6aYRjydfAbYYcXU/SjReOm3W5Tq9V48uQJz58/Z2dnh2q1eqomMM+SIOwpqVSKRCLB5OQkd+7cYXx8nPHxcc6fP08kEjmy+99FgujYDYdDWq2W7Dc8ffqU77//nlarJUk4LXiGBNF8Ee3NRCKBpmnkcjlM0ySfz5NOp0kmk0fugsONfGH4grdniIfD4RHCxPudViPnME6dBCEzG4ZBKpUiEolw8+ZNbty4QSQSkYsejUalzUXTNBzHwXEcqtUq1WqVUChEPp8nlUrJ++Ndi6soijSNXbx4kVKpRLPZlHfCaaSpp0qCCB/CCyRCz8LCAl999RWGYRCPx9F1neFweKRocxyHXq9HuVxmZ2cHTdMwDINYLCazpcOpqzgJonbIZDJMTk5KP1MgEPjv1QliR4rQMjExwfz8PIlEQtoaA4EAtVqNXq9Ht9ul2WzS7XalP6jb7VKpVCiXyxiGQTabJZVKvZW2ivcT76nrOiMjIziOg6qqjIyMSFlE1Aof8o44FRJEHI7FYnzyySckEglu3rzJ119/TSaTIZVKkU6nabVaPH/+nLW1NZrNJpubmzSbTSllu64rQ04ul5MuvEgkIh3ahyEcFrquc+PGDS5dusTKygrVapVkMkmxWGR9fV0KiB/qZJwKCSIEhUIhTNMklUrJk5DNZuXr2u021WqVzc1NarWa9BLt7++zubmJ67rouk4kEpH3Q6fT+UO1VISoYDBILpcDwHEccrkctVqNVqtFKBSSfYl/JQmKohAOhxkfHyeVSpHL5VhYWGBkZITz588TDocZDodYloVlWZTLZVZWVlheXqbZbMpGTqvVkjt9bm6OmZkZRkZGpE1GFGaDwYB6vU6xWKTf7x+RtE3TlAMk165dY3R0lO3tbUZHR7Esi/39fQ4ODmQVfZiQ4XCI67q02+33QtQHI0Hswmg0yuXLl6Wp6/bt2xQKBcLhMIZhMBgMpJS9v7/P06dPefjwIa7rYlkWvV5PVsmRSITPPvuMu3fvkkwmmZ2dZXR0VL5nv99nd3eXR48eYds26XSadDpNNBplbm6OaDRKLpfjzp079Ho9dnZ2WFlZodFosLi4yOLiorwrut3ukeep1Wq4rvtxkaCqKqFQSKadwsibzWbJZrMMBgN6vR69Xg/LsqjVatRqNRqNBs1mk16vh+u69Pt9+XNisRjpdJp8Pk88HpeuvMFgIH2o9XqdUqmEbdvysnVdF9u2abfbcmMoikKn06HZbMp0OJ1O0+l0aLfbb4WoTqfz3uqLEydBOOumpqY4c+YMmUyGW7ducenSJZl+uq5LsVjkzZs3WJbF69evZfxfX1/HcRyCwaDMfKamprh8+TKmaXLjxg0KhQKaptHv96nVapTLZV68eEG5XGZtbY3FxUUcxyEej0uyfvnlF/L5PIZhUCgUiEajBAIBUqkUqVSKYDDI9PS03Bj9fp9utyvJffbsGaVS6Z0t1b+KEyXhcFo4NTXFF198QTab5fPPP2d+fl6Kad1ul62tLX788UfK5TIvX77k1atX0lnnOA6xWIxUKkUsFuPKlSvcvXuXTCbD+Pg4hUKBwWBAtVql0Wiwvr7OvXv3WFtbo1gssrKyQqfTkR5UXdeZmpoim82Sy+W4du0a2WyWsbExzp07J//99zvddV3q9bo8BT/88AO1Wu0fr9OJkhAMBmX2kkql5BGPRqMEg0Ecx6HRaMgmvqh+HccBkBKG2MGFQkH2EpLJJPF4HECGq4ODA+r1Ont7e1QqFWq1GpZl0el05I4VzaN6vS5rif39fXq9HqFQSFbl4uswRKi0bVsOHb4PKMf9rfF/J/4VCgVmZ2cxTZM7d+7w5ZdfYhiGnC0rFos8ePCAvb09VldXWVxcxLIsWUfoun7EDn/27FlM05Q2+WAwyObmJltbWzSbTV69esXu7i6VSoXl5WVqtRrtdhvLshgMBkfUWTFQEg6HSafTcvdfvnyZeDzO5OQkExMTR3ysxWKRhw8fUiwWef36NY8fP/7TeYbjFnwnehLi8Thnzpwhm81y7tw55ufnZR4u5giWlpZYWVlhd3eX1dVVut0uhUKBfD6PaZpcvXqVmZkZ0uk0Fy5ckAZhMY+8tLTEixcvqFQqPHv2jNXVVelR+n28Fqfg8OAIwPr6OoqisLOzQ6vVwjRNWq2WrCkEVldXefDgAWtra9Lr9D5woiREo1HGxsYYGRl5Z5NdqKSO46DrOuFwmG63Sz6flw7tiYkJstmsnC1TVVXKFo7jsLGxwdbWFvV6XfqThMb0VyCs9cLbFI1GGQ6HR07C7u6uDJeu6743aePESFAUhUKhwO3bt+VUpQgzQt9PpVJ8+umnzM7Oyh2qKAqZTIZ0Oi3tKkKijkQiqKrK3t4eT548oVar8dNPP/Ho0SMcx5HE/N0BwEqlIvWkly9fvtWvOFyVv89W6ImeBJH+jY2NyRRQQKiZ2WxWzpEJ4S2VSmGa5lsNfNGssW2b3d1dDg4O2N7eZnt7W+7Mf7I7O53OkTD1oXCiJIheruM4cpxVnAT4NRyJ6czD8ddxHFqtFv1+X7oiHMeRw4Bra2v8/PPPUkcSqudp/46Kv4sTJUHY2JvNJrquy78/nKUcnjVWFIV+v8/W1hbb29vYts3W1pZ04QkLfK1W4+DgQEoKvV7voyUATpiEbrdLq9XCtm1pyv39wIZYPLGbRS5erVaxbZtSqSTtKaIpb9u27DMIR97HjBOtEy5cuMCtW7dIJpMy7z4cgoTGMxgM5CILQ5aQBMTMgeM47O/vSx1HKJheJuC4n+1ESRDKaDgcZnZ2lrm5OSKRCGNjYySTSbnru93ukWKt3W7LRT7sPz286F5efAFPFGv9fl/G7EajQaVSkV0v4a62bZtut0u5XKbRaGDbtsxShLb0MSz4P8GJnoTDv4FLDAOKfF/TNFkbDAYDOXUjdv7HnvGAR8LRfx3+/5/wEcEnwQPwSfAAfBI8AJ8ED8AnwQPwSfAAfBI8AJ8ED8AnwQPwSfAAfBI8gGNL2R+zmul1+CfBA/BJ8AB8EjwAnwQPwCfBA/BJ8AB8EjwAnwQPwCfBA/gfy3Rw+IUsISsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=2, y[i]=1, x[i]=\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=3, y[i]=1, x[i]=\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=4, y[i]=1, x[i]=\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# it's always a good idea to visualize your datapoints after loading them\n",
    "# this lets us sanity check that our labels are in fact correct\n",
    "for i in range(5):\n",
    "    print(f\"i={i}, y[i]={y[i]}, x[i]=\")\n",
    "    image = X[i].reshape([28,28])\n",
    "    fig = plt.figure(figsize=(1,1))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfde78a7",
   "metadata": {},
   "source": [
    "Looking good! We had our hypotheses ready, and now we have our data ready. Before we start training the model, we have to set aside some data to verify our model and calculate our $E_test$s for the different hypotheses. We have 14867 inputs. How should we decide how many to use for training vs testing? There are two things that we can find bound for: the generalization error $|E_\\text{in}(g) - E_\\text{out(g)}$ and $E_\\text{test}$. \n",
    "1) $|E_\\text{in}(g) - E_\\text{out(g)} = O(\\sqrroot{\\frac{\\text{log} M - text{log} \\delta}{N}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2be2af7-b4bc-4408-b5bc-4779cd8f4e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain.shape=(1000, 784)\n",
      "ytrain.shape=(1000,)\n",
      "Xtest.shape=(10000, 784)\n",
      "ytest.shape=(10000,)\n"
     ]
    }
   ],
   "source": [
    "# we now split the dataset into training a testing datasets\n",
    "# we will use a relatively small N value (just to make your future experiments faster)\n",
    "# we also have a large Ntest value, so we can be fairly confident that |Etest - Eout| is small\n",
    "N = 1000\n",
    "Ntest = 10000\n",
    "Xtrain = X[:N]\n",
    "ytrain = y[:N]\n",
    "Xtest = X[N:N+Ntest]\n",
    "ytest = y[N:N+Ntest]\n",
    "print(f\"Xtrain.shape={Xtrain.shape}\")\n",
    "print(f\"ytrain.shape={ytrain.shape}\")\n",
    "print(f\"Xtest.shape={Xtest.shape}\")\n",
    "print(f\"ytest.shape={ytest.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc413fc-ad13-43dc-883b-6378b3e7f7e6",
   "metadata": {},
   "source": [
    "## Part 3: Model Training\n",
    "\n",
    "This section introduces how to implmement and use scikit learn models.\n",
    "In general, you won't have to implement the more \"interesting\" models because they are already implemented.\n",
    "But you will have to implement the TEA model.\n",
    "The main purpose of this task is to just get you familiar with how scikit learn is structured so that you will be able to effectively use it later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9d0faad-c764-47a6-bb6b-895d39920f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime=0.5 seconds\n",
      "Ein=0.1540\n",
      "Etest=0.1504\n",
      "generalization error=0.0036\n"
     ]
    }
   ],
   "source": [
    "# in scikit learn, all learning models are implemented as a class\n",
    "# these classes are called \"estimators\",\n",
    "# and they follow the interface specified in <https://scikit-learn.org/dev/developers/develop.html>\n",
    "# in particular, all estimators will need at least three methods:\n",
    "# (1) __init__ specifies the hyperparameters to the model\n",
    "# (2) fit takes the training dataset as input and computes the hypothesis in the hypothesis class\n",
    "# (3) predict applies the hypothesis to an input datapoint or dataset\n",
    "\n",
    "class TEA:\n",
    "    def __init__(self, H):\n",
    "        self.H = H\n",
    "        self.g = None\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        d = X.shape[1]  # dimensionality of the input data\n",
    "        self.g = self.H(d)[0]\n",
    "        E_in = np.inf  \n",
    "\n",
    "        for h in self.H(d):\n",
    "            h_out = np.apply_along_axis(h,1,X)\n",
    "            this_E_in = np.mean(h_out != Y)\n",
    "            if this_E_in < E_in:  \n",
    "                self.g = h  \n",
    "                E_in = this_E_in  \n",
    "\n",
    "        # my results are\n",
    "        # for H_binary: Ein=0.4710\n",
    "        # for H_axis: 0.1540\n",
    "        # for H_axis2: 0.1540\n",
    "        # you won't be able to use the H_multiaxis2 or H_multiaxis3 hypothesis classes because of the exponential runtimes\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        assert(self.g is not None)\n",
    "        if len(X.shape) == 1:\n",
    "            return self.g(X)\n",
    "        else:\n",
    "            return np.apply_along_axis(self.g, 1, X)\n",
    "\n",
    "# we now train the model by passing the training data to the .fit method\n",
    "model = TEA(H_axis) # replace H_axis with other hypothesis classes\n",
    "time_start = time.time()\n",
    "model.fit(Xtrain, ytrain)\n",
    "time_end = time.time()\n",
    "runtime = time_end - time_start\n",
    "print(f\"runtime={runtime:0.1f} seconds\")\n",
    "\n",
    "# report the relavent metrics\n",
    "# scikit learn does not have an error metric built in, \n",
    "# but we can compute it as 1 - accuracy\n",
    "ytrain_pred = model.predict(Xtrain)\n",
    "Ein = 1 - sklearn.metrics.accuracy_score(ytrain_pred, ytrain)\n",
    "print(f\"Ein={Ein:0.4f}\")\n",
    "ytest_pred = model.predict(Xtest)\n",
    "Etest = 1 - sklearn.metrics.accuracy_score(ytest, ytest_pred)\n",
    "print(f\"Etest={Etest:0.4f}\")\n",
    "print(f'generalization error={abs(Etest - Ein):0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a24c2cd-7e25-43a7-824b-1fb5bef8fde5",
   "metadata": {},
   "source": [
    "## Part 4: Data preprocessing with random projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebc3a5bd-0688-41cf-91fe-4376a892e989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Afull.shape=(784, 784)\n"
     ]
    }
   ],
   "source": [
    "# recall that one of the most important parts of this class is learning how to effectively preprocess data\n",
    "# in the real world, you will never have to implement your own machine learning algorithms\n",
    "# existing implementations in libraries like scikit learn are highly optimized,\n",
    "# and there's no need to reinvent the wheel.\n",
    "# but you will have to preprocess data\n",
    "\n",
    "# one of the most important forms of preprocessing is the random projection\n",
    "# recall that projecting the data onto a smaller dimension (dprime) will cause:\n",
    "# (1) the runtime for TEA to go down,\n",
    "# (2) the generalization error |Ein - Eout| to go down,\n",
    "# (3) the training error Ein to go up\n",
    "# every application will have different demands on these quantities,\n",
    "# and so you will have to choose an appropriate dimension dprime for your application\n",
    "\n",
    "# to project the data, we need to generate a random d x dprime matrix\n",
    "# we use two tricks to generate the random matrix deterministically:\n",
    "# first, we set the seed; this ensures that every run of this cell will create the same matrix\n",
    "# (although the same seed will result in different matrices on different computers)\n",
    "# second, we create a full matrix and then slice it to the appropriate size\n",
    "# this ensures that the value of dprime does not affect the contents of A\n",
    "np.random.seed(0)\n",
    "d = X.shape[1]\n",
    "Afull = np.random.uniform(low=-1, high=1, size=(d, d))\n",
    "print(f'Afull.shape={Afull.shape}') # shape = d x d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8174cc89-cfd3-4374-90ab-83823dca5c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xprime.shape=(14867, 13)\n",
      "i=0, y[i]=1, x[i]=\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAAlCAYAAABmmoSAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAA5UlEQVR4nO3dMQ2DUBiFUWgqgrAz4gJ/2AAVLDhAAR7YqQWS/mmHe85MLm/gyxtp7/u+GyDC698HAH5H8BBE8BBE8BBE8BBE8BBE8BBE8BDk/fTBfd9LXrhtW8nOuq4lO8uylOyM41iyM89zyU7f919vXNdVcJKm6bquZOc8z5KdYRhKdqZpKtk5jqNk58k36IaHIIKHIIKHIIKHIIKHIIKHIIKHIIKHIIKHIIKHIIKHIIKHIIKHIIKHIIKHIIKHIIKHIK1fTUEONzwEETwEETwEETwEETwEETwEETwEETwEETwE+QCO2x1DTyXmuQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=1, y[i]=-1, x[i]=\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAAlCAYAAABmmoSAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAA50lEQVR4nO3dMQ2DUBiFUWiqgJkFE+woYMMHcrCFBBIUIIBaIOmfdrjnzOTyli9vIaG97/tugAivfx8A+B3BQxDBQxDBQxDBQxDBQxDBQxDBQ5D30weHYSh54b7vJTvbtpXsHMdRsnNdV8lO3/clO+d5fr2xLEvBSZqm67qSnXEcS3bWdS3Zmee5ZGeappKdJ9/QueEhiOAhiOAhiOAhiOAhiOAhiOAhiOAhiOAhiOAhiOAhiOAhiOAhiOAhiOAhiOAhiOAhSOtXU5DDDQ9BBA9BBA9BBA9BBA9BBA9BBA9BBA9BBA9BPgMtHUN3DYXNAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=2, y[i]=1, x[i]=\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAAlCAYAAABmmoSAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAA50lEQVR4nO3dMQ2DUBiFUWgqgBEJ4AAxaMACCZYwgAI29FAJbdI/7XDPmV8ub+ALI+1933cDRHj8+wLA7wgegggegggegggegggegggegggegjw/PbgsS8kDp2kq2dn3vWRnHMeSnXVdS3a6rivZ2bbt6415ngtu0jTneZbsXNdVslP1Lh/HUbLT933JzjAMb8/4wkMQwUMQwUMQwUMQwUMQwUMQwUMQwUMQwUMQwUMQwUMQwUMQwUMQwUMQwUMQwUMQwUOQ1q+mIIcvPAQRPAQRPAQRPAQRPAQRPAQRPAQRPAQRPAR5AUNpGkPdu7fCAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=3, y[i]=1, x[i]=\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAAlCAYAAABmmoSAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAA5klEQVR4nO3dsQ2DQBREQbDcDAESFVATJdABOc0geiCjDXJcgi35yw52Jj4tlzxdSHvf990AER7/vgDwO4KHIIKHIIKHIIKHIIKHIIKHIIKHIM9PD3ZdV/LBcRxLdqqs61qyMwxDyc51XSU7fd9/vTFN0/cXaZpmWZaSneM4SnbO8yzZ2batZGff95KdeZ7fnvHCQxDBQxDBQxDBQxDBQxDBQxDBQxDBQxDBQxDBQxDBQxDBQxDBQxDBQxDBQxDBQxDBQ5DWr6YghxcegggegggegggegggegggegggegggegggegrwANQAdQwPcEhEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=4, y[i]=1, x[i]=\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAAlCAYAAABmmoSAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAA5ElEQVR4nO3dMQ2DUBiFUWjqBAV4wAETBtDCjg5kYAQLsFMLJP3TDvecmVze8uWNr73v+26ACK9/HwD4HcFDEMFDEMFDEMFDEMFDEMFDEMFDkPfTD7uuK/nhsiwlO/u+l+xc11WyMwxDyc55niU78zx/vbGua8FJmmbbtpKdvu9LdqZpKtk5jqNkZxzHkp0n3PAQRPAQRPAQRPAQRPAQRPAQRPAQRPAQRPAQRPAQRPAQRPAQRPAQRPAQRPAQRPAQRPAQpPXUFORww0MQwUMQwUMQwUMQwUMQwUMQwUMQwUMQwUOQD2lQHUOBZmW5AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the code below plots the data points after they have been projected down into a very small dprime\n",
    "dprime = 13\n",
    "A = Afull[:, :dprime]\n",
    "Xprime = X @ A\n",
    "print(f\"Xprime.shape={Xprime.shape}\")\n",
    "for i in range(5):\n",
    "    print(f\"i={i}, y[i]={y[i]}, x[i]=\")\n",
    "    image = Xprime[i].reshape([1, dprime])\n",
    "    fig = plt.figure(figsize=(3,3))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c32f1c98-b2d8-41d1-a1da-48717613855d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime=0.0 seconds\n",
      "Ein=0.1810\n",
      "Etest=0.1810\n",
      "generalization error=0.0000\n"
     ]
    }
   ],
   "source": [
    "# the images above are no longer human interpretable;\n",
    "# remarkably, we can still learn an effective hypothesis on these newly transformed data points\n",
    "# with dprime=5, my results on H_axis2 are nearly as good as they were for the full dataset\n",
    "# on this smaller dataset size, we can actually now try the H_multiaxis* hypothesis classes as well\n",
    "\n",
    "model = TEA(H_axis)\n",
    "time_start = time.time()\n",
    "model.fit(Xtrain @ A, ytrain)\n",
    "time_end = time.time()\n",
    "runtime = time_end - time_start\n",
    "print(f\"runtime={runtime:0.1f} seconds\")\n",
    "\n",
    "ytrain_pred = model.predict(Xtrain @ A)\n",
    "Ein = 1 - sklearn.metrics.accuracy_score(ytrain_pred, ytrain)\n",
    "print(f\"Ein={Ein:0.4f}\")\n",
    "\n",
    "ytest_pred = model.predict(Xtest @ A)\n",
    "Etest = 1 - sklearn.metrics.accuracy_score(ytest, ytest_pred)\n",
    "print(f\"Etest={Etest:0.4f}\")\n",
    "\n",
    "print(f'generalization error={abs(Etest - Ein):0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fd221e",
   "metadata": {},
   "source": [
    "The parameters that were found to have the lowest $E_\\text{in}$ was with the $H_\\text{axis}$ model and $d = 13$. \n",
    "\n",
    "With this size dataset and dimensions, a simple model like $H_\\text{axis}$ is likely to prevail as a more complex model like $H_\\text{multiaxis}$ may overfit to the dataset - it will have a low $E_\\text{in}$, but a high $E_\\text{test}$. However, models as simple as $H_\\text{binary}$ will not cut it. \n",
    "\n",
    "The dimension $d = 13$ was found by simple trial-and-error. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
